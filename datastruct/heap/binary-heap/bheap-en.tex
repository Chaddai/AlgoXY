\ifx\wholebook\relax \else
% ------------------------

\documentclass{article}
%------------------- Other types of document example ------------------------
%
%\documentclass[twocolumn]{IEEEtran-new}
%\documentclass[12pt,twoside,draft]{IEEEtran}
%\documentstyle[9pt,twocolumn,technote,twoside]{IEEEtran}
%
%-----------------------------------------------------------------------------
%\input{../../../common.tex}
\input{../../../common-en.tex}

\setcounter{page}{1}

\begin{document}

\fi
%--------------------------

% ================================================================
%                 COVER PAGE
% ================================================================

\title{Binary Heaps}

\author{Larry~LIU~Xinyu
\thanks{{\bfseries Larry LIU Xinyu } \newline
  Email: liuxinyu95@gmail.com \newline}
  }

\markboth{Binary Heaps}{AlgoXY}

\maketitle

\ifx\wholebook\relax
\chapter{Binary Heaps}
\numberwithin{Exercise}{chapter}
\fi

% ================================================================
%                 Introduction
% ================================================================
\section{Introduction}
\label{introduction}

Heap is one of the elementary data structure. It is widely used
to solve some practical problems, such as sorting, prioritized
scheduling, and graph algorithms\cite{wiki-heap}.

Most popular implementations of heap are using a kind of implicit
binary heap by array, which is described in \cite{CLRS}.
Examples include C++/STL heap and Python heapq. The most efficient
heap sort algorithm is also
realized with binary heap as proposed by R. W. Floyd
\cite{wiki-heapsort} \cite{rosetta-heapsort}.

However, heaps can be general and realized with varies
of other data structures besides array.
In this chapter, explicit
binary tree is used. It leads to Leftist heaps, Skew heaps,
and Splay heaps, which are suitable for purely functional
implementation as shown
by Okasaki\cite{okasaki-book}.

A heap is a data structure that satisfies the following {\em heap property}.
\begin{itemize}
\item Top operation always returns the minimum (maximum) element;
\item Pop operation removes the top element from the heap while the heap
property should be kept, so that the new top element is still the
minimum (maximum) one;
\item Insert a new element to heap should keep the heap property. That
the new top is still the minimum (maximum) element;
\item Other operations including merge etc should all keep the heap property.
\end{itemize}

This is a kind of recursive definition, while it doesn't limit the under
ground data structure.

We call the heap with the minimum element on top as {\em min-heap},
while if the top keeps the maximum element, we call it {\em max-heap}.

% ================================================================
%                 Implicit binary heap by array
% ================================================================
\section{Implicit binary heap by array}
\label{ibheap}

Considering the heap definition in previous section, one option to
implement heap is by using trees. A straightforward solution is
to store the minimum (maximum) element in the root of the
tree, so for `top' operation, we simply return the root as the
result. And for `pop' operation, we can remove the root and
rebuild the tree from the children.

If binary tree is used to implement heap the heap, we
can call it {\em binary heap}. This chapter explains three different
realizations for binary heap.

% ================================================================
%                 Definition
% ================================================================
\subsection{Definition}

The first one is implicit binary tree. Consider the problem
how to represent a complete binary tree with array. (For example, try to
represent a complete binary tree in the programming language doesn't support structure
or record data type. Only array can be used). One solution
is to pack all elements from top level (root) down to bottom level (leaves).

Figure \ref{fig:tree-array-map} shows a complete binary tree and
its corresponding array representation.

\begin{figure}[htbp]
       \begin{center}
       	  \includegraphics[scale=0.5]{img/tree-array-map-tree.ps}
          \includegraphics[scale=0.5]{img/tree-array-map-array.ps}
        \caption{Mapping between a complete binary tree and array} \label{fig:tree-array-map}
       \end{center}
\end{figure}

This mapping between tree and array can be defined as
the following equations (The array index starts from 1).

\begin{algorithmic}[1]
\Function{PARENT}{$i$}
  \State \Return $\lfloor \frac{i}{2} \rfloor$
\EndFunction
\Statex
\Function{LEFT}{$i$}
  \State \Return $2i$
\EndFunction
\Statex
\Function{LEFT}{$i$}
  \State \Return $2i+1$
\EndFunction
\end{algorithmic}

For a given tree node which is represented as the $i$-th element of the
array, since the tree is complete, we can easily find its parent node
as the $\lfloor i/2 \rfloor$-th element; Its left child
with index of $2i$ and right child of $2i+1$. If the index
of the child exceeds the length of the array, it means this
node does not have such a child (leaf for example).

In real implementation, this mapping can be calculated fast
with bit-wise operation like the following example ANSI C code.
Note that, the array index starts from zero in C like
languages.

\lstset{language=C}
\begin{lstlisting}
#define PARENT(i) ((((i) + 1) >> 1) - 1)

#define LEFT(i) (((i) << 1) + 1)

#define RIGHT(i) (((i) + 1) << 1)
\end{lstlisting}

% ================================================================
%                 Heapify
% ================================================================
\subsection{Heapify}

The most important thing for heap algorithm is to maintain the heap
property, that the top element should be the minimum (maximum) one.

For the implicit binary heap by array, it means for a given node,
which is represented as the $i$-th index, we need develop a method
to check if both its two children conform to this property. In case
there is violation, we need swap the parent and child recursively \cite{CLRS}
to fix the problem.

Below algorithm shows the iterative solution to enforce the min-heap
property from the given index of the array.

\begin{algorithmic}[1]
\Function{HEAPIFY}{$A, i$}
  \State $n \gets |A|$
  \Loop
    \State $l \gets$ \Call{Left}{$i$}
    \State $r \gets$ \Call{Right}{$i$}
    \State $smallest \gets i$
    \If{$l < n \land A[l] < A[i]$}
      \State $smallest \gets l$
    \EndIf
    \If{$r < n \land A[r] < A[smallest]$}
      \State $smallest \gets r$
    \EndIf
    \If{$smallest \neq i$}
      \State \textproc{Exchange} $A[i] \leftrightarrow A[smallest]$
      \State $i \gets smallest$
    \Else
      \State \Return
    \EndIf
  \EndLoop
\EndFunction
\end{algorithmic}

For array $A$ and the given index $i$, None its children
should be bigger than $A[i]$, in case there is violation, we pick the smallest
element as $A[i]$, and swap the previous $A[i]$ to child.
The algorithm traverses the tree top-down to fix the heap property
until either reach a leaf or there is no heap property violation.

The \textproc{Heapify} algorithm takes $O(\lg n)$ time, where
$n$ is the number of elements. This
is because the loop time is proportion to the height of the binary tree.

When implement this algorithm, the comparison method can be passed as
a parameter, so that both min-heap and max-heap can be supported.
The following ANSI C example code uses this approach.

\begin{lstlisting}
typedef int (*Less)(Key, Key);
int less(Key x, Key y) { return x < y; }
int notless(Key x, Key y) { return !less(x, y); }

void heapify(Key* a, int i, int n, Less lt) {
    int l, r, m;
     while (1) {
        l = LEFT(i);
        r = RIGHT(i);
        m = i;
         if (l < n && lt(a[l], a[i]))
            m = l;
        if (r < n && lt(a[r], a[m]))
            m = r;
         if (m != i) {
            swap(a, i, m);
            i = m;
        }
        else
            break;
    }
}
\end{lstlisting}

Figure \ref{fig:heapify} illustrates the steps when \textproc{Heapify} processing the
array $\{16, 4, 10, 14, 7, 9, 3, 2, 8, 1\}$. The array changes to
$\{16, 14, 10, 8, 7, 9, 3, 2, 4, 1\}$.

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[scale=0.3]{img/heapify-1.ps}

    a. Step 1, 14 is the biggest element among 4, 14, and 7. Swap 4 with the left child;

    \includegraphics[scale=0.3]{img/heapify-2.ps}

    b. Step 2, 8 is the biggest element among 2, 4, and 8. Swap 4 with the right child;

    \includegraphics[scale=0.3]{img/heapify-3.ps}

    c. 4 is the leaf node. It hasn't any children. Process terminates.
    \caption{Heapify example, a max-heap case.} \label{fig:heapify}
  \end{center}
\end{figure}


% ================================================================
%                 Build a heap
% ================================================================
\subsection{Build a heap}

With \textproc{Heapify} algorithm defined, it is easy to build a heap from the arbitrary
array. Observe that the numbers of nodes in a complete binary tree
for each level is a list like below:

$1, 2, 4, 8, ..., 2^i, ...$.

The only exception is the last level. Since the tree may not full
(note that complete binary tree doesn't mean full binary tree), the
last level contains at most $2^{p-1}$ nodes, where $2^p \leq n$ and $n$
is the length of the array.

The \textproc{Heapify} algorithm doesn't have any effect on leave node.
We can skip applying \textproc{Heapify} for all leaves. In other words,
all leaf nodes have already satisfied the heap property. We only need
start checking and maintain the heap property from the last branch node.
the index of the last branch node is no greater than $\lfloor n/2 rfloor$.

Based on this fact, we can build a heap with the following algorithm.
(Assume the heap is min-heap).

\begin{algorithmic}[1]
\Function{Build-Heap}{$A$}
  \State $n \gets |A|$
  \For{$i \gets \lfloor n/2 \rfloor$ downto $1$}
    \State \Call{Heapify}{$A, i$}
  \EndFor
\EndFunction
\end{algorithmic}

Although the complexity of \textproc{Heapify} is $O(\lg n)$, the running time
of \textproc{Build-Heap} is not bound to $O(n \lg n)$ but $O(n)$. This
is a linear time algorithm. \cite{CLRS} provides the detailed proof.

Below ANSI C example program implements this heap building function.

\lstset{language=C}
\begin{lstlisting}
void build_heap(Key* a, int n, Less lt) {
    int i;
    for (i = (n-1) >> 1; i >= 0; --i)
        heapify(a, i, n, lt);
}
\end{lstlisting}

Figure \ref{fig:build-heap-1} and \ref{fig:build-heap-2}
show the steps when building a heap from
array $\{4, 1, 3, 2, 16, 9, 10, 14, 8, 7\}$.
The node in black color is the one \textproc{Heapify} being
applied, the nodes in gray color are swapped during
to keep the heap properity.

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[scale=0.5]{img/build-heap-array.ps}

    a. An array in arbitrary order before heap building process;

    \includegraphics[scale=0.3]{img/build-heap-1.ps}

    b. Step 1, The array is mapped to binary tree. The first branch node, which is
16 is examined;

    \includegraphics[scale=0.3]{img/build-heap-2.ps}

    c. Step 2, 16 is the largest element in current sub tree, next is to check node
with value 2;

    \caption{Build a heap from the arbitrary array. Gray nodes are changed in each step,
black node will be processed next step.} \label{fig:build-heap-1}
  \end{center}
\end{figure}

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[scale=0.3]{img/build-heap-3.ps}

    d. Step 3, 14 is the largest value in the sub-tree, swap 14 and 2; next is to check
node with value 3;

    \includegraphics[scale=0.3]{img/build-heap-4.ps}

    e. Step 4, 10 is the largest value in the sub-tree, swap 10 and 3; next is to check
node with value 1;

    \includegraphics[scale=0.3]{img/build-heap-5.ps}

    f. Step 5, 16 is the largest value in current node, swap 16 and 1 first; then
similarly, swap 1 and 7; next is to check the root node with value 4;

    \includegraphics[scale=0.3]{img/build-heap-6.ps}

    g. Step 6, Swap 4 and 16, then swap 4 and 14, and then swap 4 and 8;
And the whole build process finish.

    \caption{Build a heap from the arbitrary array. Gray nodes are changed in each step,
black node will be processed next step.} \label{fig:build-heap-2}
  \end{center}
\end{figure}

% ================================================================
%                 Basic heap operations
% ================================================================
\subsection{Basic heap operations}

From the generic definition of heap (not necessarily the binary heap),
It's essential to provides basic operations so that user can access
the data and modify it.

The most important operations include accessing the top element
(find the minimum or maximum one), popping the top element
from the heap, finding the top $n$ elements, decreasing a key (this
is for min-heap, and it is increasing a key for max-heap), and
insertion.

For the binary tree, most of operations are bound to $O(\lg n)$ in
worst-case, some of them, such as top is $O(1)$ constant time.

\subsubsection{Access the top element (the minimum)}
We need provide a way to access the top element efficiently.
For the binary tree realization, it is the
root stores the minimum (maximum) value. This is the first
element in the array.

\begin{algorithmic}[1]
\Function{TOP}{$A$}
  \State \Return $A[1]$
\EndFunction
\end{algorithmic}

This operation is trivial. It takes $O(1)$ time.
Here we skip the error handling for empty case. If the
heap is empty, one option is to raise an error.

\subsubsection{Heap Pop}

Pop operation is more
complex than accessing the top, because the heap property has to be maintained
after the top element is removed.

The solution is to apply \textproc{Heapify} algorithm to the
next element after the root is removed.

One simple but slow method based on this idea looka like
the following.

\begin{algorithmic}[1]
\Function{Pop-Slow}{$A$}
  \State $x \gets$ \Call{Top}{$A$}
  \State \Call{Remove}{$A$, 1}
  \If{$A$ is not empty}
    \State \Call{Heapify}{$A$, 1}
  \EndIf
  \State \Return $x$
\EndFunction
\end{algorithmic}

This algorithm firstly records the top element in $x$, then
it removes the first element from the array, the size of
this array is reduced by one. After that if the array isn't
empty, \textproc{Heapify} will applied to the new array from
the first element (It was previously the second one).

Removing the first element from array takes $O(n)$ time,
where $n$ is the length of the array. This is because
we need shift all the rest elements one by one.
This bottle neck slows the whole algorithm
to linear time.

In order to solve this problem, one alternative is
to swap the first element with the last one in the
array, then shrink the array size by one.

\begin{algorithmic}[1]
\Function{Pop}{$A$}
  \State $x \gets$ \Call{Top}{$A$}
  \State $n \gets$ \Call{Heap-Size}{$A$}
  \State \textproc{Exchange} $A[1] \leftrightarrow A[n]$
  \State \Call{Remove}{$A, n$}
  \If{$A$ is not empty}
    \State \Call{Heapify}{$A$, 1}
  \EndIf
  \State \Return $x$
\EndFunction
\end{algorithmic}

Removing the last element from the array takes
only constant $O(1)$ time, and \textproc{Heapify} is bound to $O(\lg n)$.
Thus the whole algorithm performs in $O(\lg n)$ time. The following
example ANSI C program implements this algorithm\footnote{This program does not
actually remove the last element, it reuse the last cell to store the popped
result}.

\lstset{language=C}
\begin{lstlisting}
Key pop(Key* a, int n, Less lt) {
    swap(a, 0, --n);
    heapify(a, 0, n, lt);
    return a[n];
}
\end{lstlisting}

Since the top element is removed from the array, instead swapping,
this program overwrites it with the last element before applying
\textproc{Heapify}.

\subsubsection{Find the top $k$ elements}

With pop defined, it is easy to
find the top $k$ elements from array.
we can build a max-heap from the array, then perform
pop operation $k$ times.

\begin{algorithmic}[1]
\Function{Top-k}{$A, k$}
  \State $R \gets \Phi$
  \State \Call{Build-Heap}{$A$}
  \For{$i \gets 1$ to \textproc{Min}(k, \Call{Length}{$A$})}
    \State \textproc{Append}($R$, \Call{Pop}{$A$})
  \EndFor
  \State \Return $R$
\EndFunction
\end{algorithmic}

If $k$ is greater than the length of the array,
we need return the whole array as the result. That's why it calls
the \textproc{Min} function to determine the number of loops.

Below example Python program implements the top-$k$ algorithm.

\lstset{language=Python}
\begin{lstlisting}
def top_k(x, k, less_p = MIN_HEAP):
    build_heap(x, less_p)
    return [heap_pop(x, less_p) for _ in range(min(k, len(x)))]
\end{lstlisting}

\subsubsection{Decrease key}

Heap can be used to implement priority queue. It
is important to support key modification in heap. One typical operation
is to increase the priority of a tasks so that it can be performed
earlier.

Here we present the decrease key operation for a min-heap. The
corresponding operation is increase key for max-heap.
Figure \ref{fig:decrease-key} illustrate such a case for a max-heap.
The key of the 9-th node is increased from 4 to 15.

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[scale=0.3]{img/decrease-key-a.ps}

    a. The 9-th node with key 4 will be modified;

    \includegraphics[scale=0.3]{img/decrease-key-b.ps}

    b. The key is modified to 15, which is greater than its parent;

    \includegraphics[scale=0.3]{img/decrease-key-c.ps}

    c. According the max-heap property, 8 and 15 are swapped.

    \includegraphics[scale=0.3]{img/decrease-key-d.ps}

    d. Since 15 is greater than its parent 14, they are swapped. After that, because 15 is less than 16, the process terminates.

    \caption{Example process when increase a key in a max-heap.} \label{fig:decrease-key}
  \end{center}
\end{figure}

Once a key is decreased in a min-heap, it may make
the node conflict with the heap property, that the key may be less
than some ancestor. In order to maintain the
invariant, the following auxiliary algorithm is defined to resume the heap
property.

\begin{algorithmic}[1]
\Function{Heap-Fix}{$A, i$}
  \While{$i>1 \land A[i] < A[$ \Call{Parent}{$i$} $]$}
    \State \textproc{Exchange} $A[i] \leftrightarrow A[$ \Call{Parent}{$i$} $]$
    \State $i \gets$  \Call{Parent}{$i$}
  \EndWhile
\EndFunction
\end{algorithmic}

This algorithm repeatedly compares the keys of parent node and
current node. It swap the nodes if
the parent contains the smaller key. This process is performed
from the current node towards the root node till it finds that
the parent node holds the smaller key.

With this auxiliary algorithm, decrease key can be realized
as below.

\begin{algorithmic}[1]
\Function{Decrease-Key}{$A, i, k$}
  \If{$k < A[i]$}
    \State $A[i] \gets k$
    \State \Call{Heap-Fix}{$A, i$}
  \EndIf
\EndFunction
\end{algorithmic}

This algorithm is only triggered when the new key
is less than the original key. The performance is bound to $O(\lg n)$.
Below example ANSI C program implments the algorithm.

\lstset{language=C}
\begin{lstlisting}
void heap_fix(Key* a, int i, Less lt) {
    while (i > 0 && lt(a[i], a[PARENT(i)])) {
        swap(a, i, PARENT(i));
        i = PARENT(i);
    }
}

void decrease_key(Key* a, int i, Key k, Less lt) {
    if (lt(k, a[i])) {
        a[i] = k;
        heap_fix(a, i, lt);
    }
}
\end{lstlisting}

\subsubsection{Insertion}

In some materials like \cite{CLRS}, insertion is implemented by using \textproc{Decrease-Key}.
A new node with $\infty$ as key is created. According
to the min-heap property, it should be the last element
in the under ground array. After that, the key is decreased to
the value to be inserted, and \textproc{Decrease-Key} is called
to fix any violation to the heap property.

Alternatively, we can reuse \textproc{Heap-Fix} to implement
insertion. The new key is directly appended at the end of the array,
and the \textproc{Heap-Fix} is applied to this new node.

\begin{algorithmic}[1]
\Function{Heap-Push}{$A, k$}
  \State \Call{Append}{$A, k$}
  \State \Call{Heap-Fix}{$A, |A|$}
\EndFunction
\end{algorithmic}

The following example Python program implements the heap insertion
algorithm.

\lstset{language=Python}
\begin{lstlisting}
def heap_insert(x, key, less_p = MIN_HEAP):
    i = len(x)
    x.append(key)
    heap_fix(x, i, less_p)
\end{lstlisting}

% ================================================================
%                 Heap sort
% ================================================================
\subsection{Heap sort}
\label{heap-sort}

Heap sort is interesting application of heap. According
to the heap property, the min(max) element can be easily accessed
by from the top of the heap. A straightforward way to sort a list
of values is to build a heap from them, then continuously
pop the smallest element till the heap is empty.

The algorithm based on this idea can be defined like below.

\begin{algorithmic}[1]
\Function{Heap-Sort}{$A$}
  \State $R \gets \Phi$
  \State \Call{Build-Heap}{$A$}
  \While{$A \neq \Phi$}
    \State \textproc{Append}($R$, \Call{Heap-Pop}{$A$})
  \EndWhile
  \State \Return $R$
\EndFunction
\end{algorithmic}

The following Python example program implements this definition.

\lstset{language=Python}
\begin{lstlisting}
def heap_sort(x, less_p = MIN_HEAP):
    res = []
    build_heap(x, less_p)
    while x!=[]:
        res.append(heap_pop(x, less_p))
    return res
\end{lstlisting}

When sort $n$ elements, the \textproc{Build-Heap} is bound to $O(n)$.
Since pop is $O(\lg n)$, and it
is called $n$ times, so the overall sorting takes $O(n \lg n)$
time to run. Because we use another list to hold the result,
the space requirement is $O(n)$.

Robert. W. Floyd found a fast implementation of heap sort.
The idea is to build a max-heap instead of min-heap, so the first
element is the biggest one. Then the biggest element is swapped
with the last element in the array, so that it is in the right
position after sorting. As the last element becomes the new top,
it may violate the heap property. We can shrink the heap size
by one and perform
\textproc{Heapify} to resume the heap property.
This process
is repeated till there is only one element left in the heap.

\begin{algorithmic}[1]
\Function{Heap-Sort}{$A$}
  \State \Call{Build-Max-Heap}{$A$}
  \While{$|A| > 1$}
    \State \textproc{Exchange} $A[1] \leftrightarrow A[n]$
    \State $|A| \gets |A| - 1$
    \State \Call{Heapify}{$A, 1$}
  \EndWhile
\EndFunction
\end{algorithmic}

This is in-place algorithm, it needn't any extra spaces to hold
the result. The folloing. The following ANSI C example code
implements this algorithm.

\lstset{language=C}
\begin{lstlisting}
void heap_sort(Key* a, int n) {
    build_heap(a, n, notless);
    while(n > 1) {
        swap(a, 0, --n);
        heapify(a, 0, n, notless);
    }
}
\end{lstlisting}

\begin{Exercise}
\begin{itemize}
\item Somebody considers one alternative to realize in-place heap sort. Take
sorting the array in ascending order as example, the first step is to build
the array as a minimum heap $A$, but not the maximum heap like the Floyd's method.
After that the first element $a_1$ is in the correct place. Next, treat
the rest $\{a_2, a_3, ..., a_n\}$ as a new heap, and perform
\textproc{Heapify} to them from $a_2$ for these $n-1$ elements. Repeating this
advance and \textproc{Heapify} step from left to right would sort the array.
The following example ANSI C code illustrates this idea.
Is this solution correct? If yes, prove it; if not, why?
\lstset{language=C}
\begin{lstlisting}
void heap_sort(Key* a, int n) {
    build_heap(a, n, less);
    while(--n)
        heapify(++a, 0, n, less);
}
\end{lstlisting}

\item Because of the same reason, can we perform \textproc{Heapify} from
left to right $k$ times to realize in-place top-$k$ algorithm like below
ANSI C code?
\lstset{language=C}
\begin{lstlisting}
int tops(int k, Key* a, int n, Less lt) {
    build_heap(a, n, lt);
    for (k = MIN(k, n) - 1; k; --k)
        heapify(++a, 0, --n, lt);
    return k;
}
\end{lstlisting}
\end{itemize}
\end{Exercise}

% ================================================================
%                 Explicit binary heap
% ================================================================
\section{Leftist heap and Skew heap, explicit binary heaps}
\label{ebheap}

Instead of using implicit binary tree by array, it is natural to
consider why we can't use explicit binary tree to realize heap?

There are some problems must be solved if we turn into explicit
binary tree as the under ground data strucutre for heap.

The first problem is about the $HEAP-POP$ or $DELETE-MIN$ operation.
If the explicit binary tree is represent as the form of
(left value right), which is shown in figure \ref{fig:lvr}

\begin{figure}[htbp]
       \begin{center}
       	  \includegraphics[scale=1]{img/lvr.ps}
        \caption{A binary tree, all values in left child and right child are smaller than $k$.} \label{fig:lvr}
       \end{center}
\end{figure}

If $k$ is the top element, all values in left and right children are less
than $k$. After $k$ is popped, only left and right children are left.
They have to be merged to a new tree. Since heap property should be maintained
after merge, so the new root element is the smallest one.

Since both left child and right child are heaps in binary tree, the two trivial
cases can be found immediately.

\begin{algorithmic}[1]
\Function{MERGE}{$L, R$}
  \If{$L = NIL$}
    \State \Return $R$
  \ElsIf{$R = NIL$}
    \State \Return $L$
  \Else
    \State $...$
  \EndIf
\EndFunction
\end{algorithmic}

If neither left child nor right child is empty tree, because they all fit
heap property, the top element of them are all minimum value respectively.
One solution is to compare the root value of the left and right children,
select the smaller one as the new root of the merged heap, and recursively
merget the other child to one of the children of the smaller one.
For instance if $L = (A x B)$ and $R = (A' y B')$, where $A, A', B, B'$
are all sub trees, and $x < y$. There are two candidate results according
to this strategy.

\begin{itemize}
\item $(MERGE(A, R) x B)$
\item $(A x MERGE(B, R))$
\end{itemize}

Both are correct result. One simplified solution is only merge on right
sub tree. Leftist tree provides a systematically approach based on this
idea.

% ================================================================
%                 Definition
% ================================================================
\subsection{Definition}

The heap implemented by Leftist tree is called Leftist heap. Leftist
tree is first introduced by C. A. Crane in 1972\cite{wiki-leftist-tree}.

\subsubsection{Rank (S-value)}

In Leftist tree, a rank value (or $S$ value) is defined to each node.
Rank is the distance to the nearest external node. Where external node
is a NIL concept extended from leaf node.

For example, in figure \ref{fig:rank}, the rank of NIL
is defined 0, consider the root node 4, The nearest leaf node is
the child of node 8. So the rank of root node 4 is 2. Because node
6 and node 8 are all only contain NIL, so the rank values are 1.
Although node 5 has non-NIL left child, However, since the right
child is NIL, so the rank value, which is the minimum distance
to leaf is still 1.

\begin{figure}[htbp]
   \begin{center}
     \includegraphics[scale=0.5]{img/rank.ps}
     \caption{rank(4) = 2, rank(6) = rank(8) = rank(5) = 1.} \label{fig:rank}
   \end{center}
\end{figure}

\subsubsection{Leftist property}

With rank defined, we can create a strategy when merging.

\begin{itemize}
\item Every time when merging, we always merge to right child; Denote the rank
of the new right sub tree as $r_r$;
\item Compare the ranks of the left and right children, if the rank of
left sub tree is $r_l$ and $r_l < r_r$, we swap the left and right children.
\end{itemize}

We call this `Leftist property'. Generally speaking, a Leftist tree always
has the shortest path to an external node on the right.

A Leftist tree tends to be very unbalanced, However, it ensures an important
property as specified in the following theorem.

If a Leftist tree $T$ contains $N$ internal nodes, the path from root to the
rightmost external node contains at most $\lfloor \log{(N+1)} \rfloor$ nodes.

For the proof of these theorem, please refer to \cite{brono-book} and \cite{TAOCP}.

With this theorem, algorithms operate along this path are all bound to $O(\lg N)$.

\subsection*{Definition of Leftist heap in Haskell}

In Haskell the definition of Leftist tree is almost as same as the
binary search tree except there is a rank field added.

\lstset{language=Haskell}
\begin{lstlisting}
data LHeap a = E -- Empty
             | Node Int a (LHeap a) (LHeap a) -- rank, element, left, right
               deriving (Eq, Show)
\end{lstlisting}

In order to access the rank field, a helper function is provided.

\begin{lstlisting}
rank::LHeap a -> Int
rank E = 0
rank (Node r _ _ _) = r
\end{lstlisting}

\subsection*{Definition of Leftist heap in Scheme/Lisp}

In Scheme/Lisp, list is used to represent the Leftist tree. In order
to output the tree easily in in-order format, a node is arranged as
(left rank element right). Some auxiliary functions are defined
to access these fields of a node.

\lstset{language=lisp}
\begin{lstlisting}
(define (left t)
  (if (null? t) '() (car t)))

(define (rank t)
  (if (null? t) 0 (cadr t)))

(define (elem t)
  (if (null? t) '() (caddr t)))

(define (right t)
  (if (null? t) '() (cadddr t)))
\end{lstlisting}

And a construction function is provided so that a node can be
built explicitly.

\begin{lstlisting}
(define (make-tree l s x r) ;; l: left, s: rank, x: elem, r: right
  (list l s x r))
\end{lstlisting}

% ================================================================
%                 Merge
% ================================================================
\subsection{Merge}

In order to realize `merge', an auxiliary algorithm is given to
compare the ranks and do swapping if necessary.

\begin{algorithmic}[1]
\Function{LEFTIFY}{$T$}
  \State $l \gets LEFT(T), r \gets RIGHT(T)$
  \State $k \gets KEY(T)$
  \If{$RANK(l) < RANK(r)$}
    \State $RANK(T) \gets RANK(L) + 1$
  \Else
    \State $RANK(T) \gets RANK(R) + 1$
    \State Exchange $l \leftrightarrow r$
  \EndIf
\EndFunction
\end{algorithmic}

The algorithm compares the rank of the left and
right sub trees, pick the less one and add it by one as the
rank of the modified node. If the rank of left side is greater,
it will also swap the left and right children.

The reason why rank need to be increased by one is because there
is a new key added on top of the tree, which causes the rank
increase.

With $LEFTIFY$ defined, merge algorithm can be provided as the
following.

\begin{algorithmic}[1]
\Function{MERGE}{$L, R$}
  \If{$L = NIL$}
    \State \Return $R$
  \ElsIf{$R = NIL$}
    \State \Return $L$
  \Else
    \State $T \gets CREATE-NEW-NODE()$
    \If{$KEY(L) < KEY(R)$}
      \State $KEY(T) \gets KEY(L)$
      \State $LEFT(T) \gets LEFT(L)$
      \State $RIGHT(T) \gets MERGE(RIGHT(L), R)$
    \Else
      \State $KEY(T) \gets KEY(R)$
      \State $LEFT(T) \gets LEFT(R)$
      \State $RIGHT(T) \gets MERGE(L, RIGHT(R))$
    \EndIf
    \State $LEFTIFY(T)$
  \EndIf
  \State \Return $T$
\EndFunction
\end{algorithmic}

Note that $MERGE$ algorithm always operate on right side, and call
$LEFITFY$ to ensure the `Leftist property, so that this algorithm
is bound to $O(\lg N)$.

\subsection*{Merge in Haskell}

Translate the algorithm to Haskell lead to the following program.
Here we modified the pseudo code to a pure functional style.

\lstset{language=Haskell}
\begin{lstlisting}
merge::(Ord a)=>LHeap a -> LHeap a -> LHeap a
merge E h = h
merge h E = h
merge h1@(Node _ x l r) h2@(Node _ y l' r') =
    if x < y then makeNode x l (merge r h2)
    else makeNode y l' (merge h1 r')

makeNode::a -> LHeap a -> LHeap a -> LHeap a
makeNode x a b = if rank a < rank b then Node (rank a + 1) x b a
                 else Node (rank b + 1) x a b
\end{lstlisting}

\subsection*{Merge in Scheme/Lisp}

In Scheme/Lisp, the $LEFTIFY$ algorithm can be defined as an
inner function inside $MERGE$.

\lstset{language=lisp}
\begin{lstlisting}
(define (merge t1 t2)
  (define (make-node x a b)
    (if (< (rank a) (rank b))
	(make-tree b (+ (rank a) 1) x a)
	(make-tree a (+ (rank b) 1) x b)))
  (cond ((null? t1) t2)
	((null? t2) t1)
	((< (elem t1) (elem t2)) (make-node (elem t1) (left t1) (merge (right t1) t2)))
	(else (make-node (elem t2) (left t2) (merge t1 (right t2))))))
\end{lstlisting}

\subsubsection{Merge operation in implicit binary heap by array}

In most cases implicit binary heap by array performs very fast, and
it fits modern computer with cache technology well. However, merge
is the algorithm bounds to $O(N)$ time. The best you can do is
concatenate two arrays together and make a heap of the result \cite{NIST}.

\begin{algorithmic}[1]
\Function{MERGE-HEAP}{$A, B$}
  $C \gets CONCAT(A, B)$
  $BUILD-HEAP(C)$
\EndFunction
\end{algorithmic}

We omit the implementation of this algorithm in C++ and Python because
they are trivial.

% ================================================================
%                 Basic heap operations
% ================================================================
\subsection{Basic heap operations}

Most of the basic heap operations can be implemented easily with $MERGE$
algorithm define above.

\subsubsection{Find minimum (top) and delete minimum (pop)}
Since we keep the smallest element in root node, finding the minimum
value (top element) is trivial. It's a $O(1)$ operation.

\begin{algorithmic}[1]
\Function{TOP}{$T$}
  \State \Return $KEY(T)$
\EndFunction
\end{algorithmic}

While if the top element popped, left and right children are merged
so the heap updated.

\begin{algorithmic}[1]
\Function{POP}{$T$}
  \State \Return $MERGE(LEFT(T), RIGHT(T))$
\EndFunction
\end{algorithmic}

Note that pop operation on Leftist heap takes $O(\lg N)$ time.

\subsubsection*{Find minimum (top) and delete minimum in Haskell}

We skip the error handling of operation on an empty
Leftist heap.

\lstset{language=Haskell}
\begin{lstlisting}
findMin :: LHeap a -> a
findMin (Node _ x _ _) = x
\end{lstlisting}

\begin{lstlisting}
deleteMin :: (Ord a) => LHeap a -> LHeap a
deleteMin (Node _ _ l r) = merge l r
\end{lstlisting}

\subsubsection*{Find minimum (top) and delete minimum in Scheme/Lisp}

With merge function defined, these operations are trivial to implement
in Scheme/Lisp.

\lstset{language=lisp}
\begin{lstlisting}
(define (find-min t)
  (elem t))

(define (delete-min t)
  (merge (left t) (right t)))
\end{lstlisting}

\subsubsection{Insertion}

To insert a new key to the heap, one solution is to create a single
leaf node from the key, and perform merge with this leaf node and
the Leftist tree.

\begin{algorithmic}[1]
\Function{INSERT}{$T, k$}
  \State $x \gets CREATE-NEW-NODE()$
  \State $KEY(x) \gets k$
  \State $RAKN(x) \gets 1$
  \State $LEFT(x), RIGHT(x) \gets NIL$
  \State \Return $MERGE(x, T)$
\EndFunction
\end{algorithmic}

Since insert still call merge inside, the algorithm is bound to $O(\lg N)$
time.

\subsubsection*{Insertion in Haskell}

Translating the above algorithm to Haskell is trivial.

\lstset{language=Haskell}
\begin{lstlisting}
insert::(Ord a)=> LHeap a -> a -> LHeap a
insert h x = merge (Node 1 x E E) h
\end{lstlisting}

In order to provide a convenient way to build a Leftist heap from
a list, an auxiliary function is given as the following.

\begin{lstlisting}
fromList :: (Ord a) => [a] -> LHeap a
fromList = foldl insert E
\end{lstlisting}

This function can be used like this.

\begin{lstlisting}
fromList [9, 4, 16, 7, 10, 2, 14, 3, 8, 1]
\end{lstlisting}

It will create a Leftist heap as below.

\begin{verbatim}
Node 1 1 (Node 3 2 (Node 2 4 (Node 2 7 (Node 1 16 E E)
(Node 1 10 E E)) (Node 1 9 E E)) (Node 2 3 (Node 1 14 E E)
(Node 1 8 E E))) E
\end{verbatim}

Figure \ref{fig:leftist-tree} shows the result respectively.

\begin{figure}[htbp]
   \begin{center}
   	  \includegraphics[scale=0.5]{img/leftist-tree.ps}
    \caption{A Leftist tree.} \label{fig:leftist-tree}
   \end{center}
\end{figure}

\subsubsection*{Insertion in Scheme/Lisp}

Based on the algorithm, once inserting a new element, a leaf node
is created and merge to the heap.

\lstset{language=lisp}
\begin{lstlisting}
(define (insert t x)
  (merge (make-tree '() 1 x '()) t))
\end{lstlisting}

This function can be verified by continuously insert all elements
from a list so that a Leftist heap can be built as a result.

\begin{lstlisting}
(define (from-list lst)
  (fold-left insert '() lst))
\end{lstlisting}

One example test case which is equivalent to the Haskell program
is shown like the following.

\begin{lstlisting}
(define (test-from-list)
  (from-list '(16 14 10 8 7 9 3 2 4 1)))
\end{lstlisting}

Evaluate this function yields a Leftist tree, which can be output
in in-order as below.

\begin{lstlisting}
((((((((() 1 16 ()) 1 14 ()) 1 10 ()) 1 8 ()) 2 7 (() 1 9 ())) 1 3
()) 2 2 (() 1 4 ())) 1 1 ())
\end{lstlisting}

% ================================================================
%                 Heap sort
% ================================================================
\subsection{Heap sort by Leftist Heap}

With all the basic operations defined, it's straightforward to
implement heap sort in a $N$-popping way. Once we need sort a
list of elements, we first build a Leftist heap from the list.
Then repeatedly pop the minimum element from the heap until heap
is empty.

First is the algorithm to build the Leftist heap by insert all
elements to an empty heap.

\begin{algorithmic}[1]
\Function{BUILD-HEAP}{$A$}
  \State $H \gets NIL$
  \For{each $x$ in $A$}
    \State $T \gets INSERT(T, x)$
  \EndFor
  \State \Return $T$
\EndFunction
\end{algorithmic}

And the heap sort algorithm is as same as the generic one presented
in \ref{heap-sort}.

Note this algorithm is bound to $O(N \lg N)$ time.

\subsection*{Heap sort in Haskell}

In Haskell program, since we have already defined the `fromList'
auxiliary function to build Leftist heap from a list, the heap sort
algorithm can utilize it.

\lstset{language=Haskell}
\begin{lstlisting}
heapSort :: (Ord a) => [a] -> [a]
heapSort = hsort . fromList where
    hsort E = []
    hsort h = (findMin h):(hsort $ deleteMin h)
\end{lstlisting} %$

Here is an example case which is used in previous C++ and Python
programs.

\begin{lstlisting}
heapSort [16, 14, 10, 8, 7, 9, 3, 2, 4, 1]
\end{lstlisting}

It will output the same result as the following.

\begin{verbatim}
[1,2,3,4,7,8,9,10,14,16]
\end{verbatim}

\subsection*{Heap sort in Scheme/Lisp}

In Scheme/Lisp program, we can first use `from-list' function
to turn a list of element into a Leftist heap, then repeatedly
pop the smallest one to a result list.

\lstset{language=lisp}
\begin{lstlisting}
(define (heap-sort lst)
  (define (hsort t)
    (if (null? t) '() (cons (find-min t) (hsort (delete-min t)))))
  (hsort (from-list lst)))
\end{lstlisting}

Here is a simple test case.

\begin{lstlisting}
heap-sort '(16 14 10 8 7 9 3 2 4 1))
\end{lstlisting}

Evaluate the case will out put an ordered list.

\begin{lstlisting}
(1 2 3 4 7 8 9 10 14 16)
\end{lstlisting}

% ================================================================
%                 Skew Heap
% ================================================================


\subsection{Skew heaps}
\label{skew-heap}

The problem with Leftist heap is that, it performs bad in some cases.
For example, if we examine the Leftist heap behind the above heap
sort test case, it's a very unbalanced binary tree as shown in figure
\ref{fig:unbalanced-leftist-tree}\footnote{run Haskell Leftist tree
function: fromList [16, 14, 10, 8, 7, 9, 3, 2, 4, 1] will generates
the result: Node 1 1 (Node 2 2 (Node 1 3 (Node 2 7 (Node 1 8 (Node 1 10 (Node 1 14 (Node 1 16 E E) E) E) E) (Node 1 9 E E)) E) (Node 1 4 E E)) E}.

\begin{figure}[htbp]
   \begin{center}
   	  \includegraphics[scale=0.3]{img/unbalanced-leftist-tree.ps}
    \caption{A very unbalanced Leftist tree build from list [16, 14, 10, 8, 7, 9, 3, 2, 4, 1].} \label{fig:unbalanced-leftist-tree}
   \end{center}
\end{figure}

The binary tree is almost turned to be a linked-list. The worst case
is when feed a ordered list for building Leftist tree, since the
tree changed to linked-list, the time bound degrade from $O(\lg N)$
to $O(N)$.

Skew heap (or {\em self-adjusting heap}) is one step ahead simplified Leftist heap \cite{wiki-skew-heap} \cite{self-adjusting-heaps}.

Remind the Leftist heap, we swap the left and right children during merge
when the rank on left side is less than right side. This comparison strategy
doesn't work when one of the sub tree has only one child. Because
in such case, the rank of the sub tree is always 1 no matter how
big it is. A Brute-force approach is to swap the left and right children
every time when merge. This idea leads to Skew heap.

\subsubsection{Definition of Skew heap}

A Skew heap is a heap implemented with Skew tree. A Skew tree is a special
binary tree. The minimum element is stored in root node. Every sub tree is
also a skew tree.

Based on above discussion, there is no use to keep the rank (or $S$-value)
field, so the Skew heap definition is as same as the binary tree from the
programming language point of view.

\subsubsection*{Definition of Skew heap in Haskell}

After removing rank from Leftist heap definition, we can get the Skew
heap one.

\lstset{language=Haskell}
\begin{lstlisting}
data SHeap a = E -- Empty
             | Node a (SHeap a) (SHeap a) -- element, left, right
               deriving (Eq, Show)
\end{lstlisting}

\subsubsection*{Definition of Skew heap in Scheme/Lisp}
\label{skew-heap-def-lisp}

Since Skew heap is just a special kind of binary tree, the definition is
as same as binary tree's. In Scheme/Lisp, the inner data structure is
list. We organize it in in-order for easy output purpose.

Some auxiliary access functions and a simple constructor is provided.

\lstset{language = lisp}
\begin{lstlisting}
(define (left t)
  (if (null? t) '() (car t)))

(define (elem t)
  (if (null? t) '() (cadr t)))

(define (right t)
  (if (null? t) '() (caddr t)))

;; constructor
(define (make-tree l x r) ;; l: left, x: element, r: right
  (list l x r))
\end{lstlisting}

\subsubsection{Merge}

The merge algorithm tends to be very simple. When we merge two Skew
trees, we compare the root element of each tree, and pick the smaller
one as the new root, we then merge the other tree contains bigger
element onto the right sub tree and swap the left and right children.

\begin{algorithmic}[1]
\Function{MERGE}{$L, R$}
  \If{$L = NIL$}
    \State \Return $R$
  \ElsIf{$R = NIL$}
    \State \Return $L$
  \Else
    \State $T \gets CREATE-EMPTY-NODE()$
    \If{$KEY(L) < KEY(R)$}
      \State $KEY(T) \gets KEY(L)$
      \State $LEFT(T) \gets MERGE(R, RIGHT(L))$
      \State $RIGHT(T) \gets LEFT(L)$
    \Else
      \State $KEY(T) \gets KEY(R)$
      \State $LEFT(T) \gets MERGE(L, RIGHT(R))$
      \State $RIGHT(T) \gets LEFT(R)$
    \EndIf
    \State \Return $T$
  \EndIf
\EndFunction
\end{algorithmic}

\subsubsection*{Skew heap in Haskell}

Translating the above algorithm into Haskell gets a simple merge program.

\lstset{language=Haskell}
\begin{lstlisting}
merge::(Ord a)=>SHeap a -> SHeap a -> SHeap a
merge E h = h
merge h E = h
merge h1@(Node x l r) h2@(Node y l' r') =
    if x < y then Node x (merge r h2) l
    else Node y (merge h1 r') l'
\end{lstlisting}

All the rest programs are as same as the Leftist heap except we needn't
provide rank value when construct a node.

\begin{lstlisting}
insert::(Ord a)=> SHeap a -> a -> SHeap a
insert h x = merge (Node x E E) h

findMin :: SHeap a -> a
findMin (Node x _ _) = x

deleteMin :: (Ord a) => SHeap a -> SHeap a
deleteMin (Node _ l r) = merge l r
\end{lstlisting}

If we feed a completely ordered list to Skew heap, it will results a
fairly balanced binary trees as shown in figure \ref{fig:skew-tree}.

\begin{lstlisting}
*SkewHeap>fromList [1..10]
Node 1 (Node 2 (Node 6 (Node 10 E E) E) (Node 4 (Node 8 E E) E))
(Node 3 (Node 5 (Node 9 E E) E) (Node 7 E E))
\end{lstlisting}

\begin{figure}[htbp]
   \begin{center}
   	  \includegraphics[scale=0.5]{img/skew-tree.ps}
    \caption{Skew tree is still balanced even the input is an ordered list.} \label{fig:skew-tree}
   \end{center}
\end{figure}

\subsubsection*{Skew heap in Scheme/Lisp}

In merge program, if any one of the tree to be merged, it just returns the
other one. For non-trivial case, the program select the smaller one
as the new root element, merge the tree contains the bigger element
to the right child, then swap the two children.

\lstset{language = lisp}
\begin{lstlisting}
(define (merge t1 t2)
  (cond ((null? t1) t2)
	((null? t2) t1)
	((< (elem t1) (elem t2))
	 (make-tree (merge (right t1) t2)
		    (elem t1)
		    (left t1)))
	(else
	 (make-tree (merge t1 (right t2))
		    (elem t2)
		    (left t2)))))
\end{lstlisting}

With merge function defined, insert can be treated as just a special
merge case, that one tree is a leaf which contains the value to be
inserted.

\begin{lstlisting}
(define (insert t x)
  (merge (make-tree '() x '()) t))
\end{lstlisting}

The find minimum, delete minimum and heap sort functions are all
as same as the leftist heap programs, that they can be put to
a generic module.

We only show the testing result in this section.

\begin{lstlisting}
(load "skewheap.scm")
;Loading "skewheap.scm"... done
;Value: insert

(test-from-list)
;Value 13: (((() 4 ()) 2 (((() 9 ()) 7 ((((() 16 ()) 14 ()) 10 ())
8 ())) 3 ())) 1 ())

(test-sort)
;Value 14: (1 2 3 4 7 8 9 10 14 16)
\end{lstlisting}

% ================================================================
%                 Splay Heap
% ================================================================

\section{Splay heap, another explicit binary heap}
\label{splayheap}

Leftist heap presents that it's quite possible to implement
heap data structure with explicit binary tree. Skew heap
shows one method to solve the balance problem. Splay heap
on the other hand, shows another balance approach.

Although Leftist heap and Skew heap use binary trees, they
are not Binary Search tree (BST). If we turn the underground
data structure to binary search tree, the minimum(maximum)
element isn't located in root node. It takes $O(\lg N)$ time
to find the minimum(maximum) element.

Binary search tree becomes inefficient if it isn't well
balanced, operations degrades to $O(N)$ in the worst case.
Although it's quite OK to use red-black tree to implement
binary heap, Splay tree provides a light weight implementation
with acceptable dynamic balancing result.

% ================================================================
%                 Definition
% ================================================================
\subsection{Definition}

Splay tree uses a cache-like approach that it keeps rotating the current
access node close to the top, so that the node can be accessed fast
next time. It defines such kinds of operation as ``Splay''. For an
unbalanced binary search tree, after several times of splay operation, the
tree tends to be more and more balanced. Most basic operation of
Splay tree have amortized $O(\lg N)$ time. Splay tree was invented
by Daniel Dominic Sleator and Robert Endre Tarjan in 1985\cite{wiki-splay-tree}
\cite{self-adjusting-trees}.

\subsubsection{Splaying}

There are two kinds of splaying approach method. The first one is
a bit complex, However, it can be implemented fairly simple with
pattern matching. The second one is simple, but the implementation
is a bit complex.

Denote the node is currently accessed as $X$, it's parent node as $P$,
and it's grand parent node (if has) as $G$. There are 3 steps for
splaying. Each step contains 2 symmetric cases. For illustration
purpose, only one case is shown for each step.

\begin{itemize}
\item {\em Zig-zig step.} As shown in figure \ref{fig:zig-zig}, in this case,
both $X$, and its parent $P$ are either left children or right children. By
rotating 2 times, X becomes the new root.

\begin{figure}[htbp]
   \begin{center}
   	  \includegraphics[scale=0.5]{img/zig-zig-a.ps}
          \includegraphics[scale=0.5]{img/zig-zig-b.ps}
          \caption{Zig-zig case.} \label{fig:zig-zig}
   \end{center}
\end{figure}

\item {\em Zig-zag step.} As shown in figure \ref{fig:zig-zag}, in this
case, $X$ is the right child of its parent while $P$ is the left child.
Or $X$ is the left child of $P$, and $P$ is the right child of $G$.
After rotation, $X$ becomes the new root, $P$ and $G$ become siblings.

\begin{figure}[htbp]
   \begin{center}
   	  \includegraphics[scale=0.5]{img/zig-zag-a.ps}
          \includegraphics[scale=0.5]{img/zig-zag-b.ps}
          \caption{Zig-zag case.} \label{fig:zig-zag}
   \end{center}
\end{figure}

\item {\em Zig step.} As shown in figure \ref{fig:zig}, in this case,
$P$ is the root, we perform one rotate, so that $X$ becomes new root.
Note this is the last step in splay operation.

\begin{figure}[htbp]
   \begin{center}
   	  \includegraphics[scale=0.5]{img/zig-a.ps}
          \includegraphics[scale=0.5]{img/zig-b.ps}
          \caption{Zig case.} \label{fig:zig}
   \end{center}
\end{figure}

\end{itemize}

Okasaki found a simple rule for Splaying \cite{okasaki-book},
that every time we follow
two left branches in a row, or two right branches in a row, we rotate
those two nodes.

Based on this rule, the Splaying can be realized in such a way.
When we access node for a key $x$ (can be during the process of
inserting a node, or looking up a node, or deleting a node), if
we found we traverse two left branches (or two right branches), we
partition the tree in two part $L$ and $R$, where $L$ contains all
nodes smaller than $x$, and $R$ contains all nodes bigger than $x$.
We can then create a new tree (for instance in insertion),
with $x$ as the root, $L$ as the left child and $R$ is the right child.
Note the partition process is recursive, because it will do splaying
inside.

\begin{algorithmic}[1]
\Function{PARTITION}{$T, pivot$}
  \If{$T = NIL$}
    \State \Return $(NIL, NIL)$
  \EndIf
  \State $x \gets KEY(T)$
  \State $l \gets LEFT(T)$
  \State $r \gets RIGHT(T)$
  \State $L \gets NIL$
  \State $R \gets NIL$
  \If{$x < pivot$}
    \If{$r = NIL$}
      \State $L \gets T$
    \Else
      \State $x' \gets KEY(r)$
      \State $l' \gets LEFT(r)$
      \State $r' \gets RIGHT(r)$
      \If{$x' < pivot$}
        \State $(small, big) \gets PARTITION(r', pivot)$
        \State $L \gets CREATE-NODE(CREATE-NODE(l, x, r), x', small)$
        \State $R \gets big$
      \Else
        \State $(small, big) \gets PARTITION(l', pivot)$
        \State $L \gets CREATE-NODE(l, x, small)$
        \State $R \gets CREATE-NODE(big, x', r')$
      \EndIf
    \EndIf
  \Else
    \If{$l = NIL$}
      \State $L \gets NIL$
    \Else
      \State $x' \gets KEY(L)$
      \State $l' \gets LEFT(L)$
      \State $r' \gets RIGHT(L)$
      \If{$x' > pivot$}
        \State $(small, big) \gets PARTITION(l', pivot)$
        \State $L \gets small$
        \State $R \gets CREATE-NODE(l', x', CREATE-NODE(r', x, r))$
      \Else
        \State $(small, big) \gets PARTITION(r', pivot)$
        \State $L \gets CREATE-NODE(l', x, small)$
        \State $R \gets CREATE-NODE(big, x, r)$
      \EndIf
    \EndIf
  \EndIf
  \State \Return $(L, R)$
\EndFunction

\Function{CREATE-NODE}{$l, x, r$}
  \State $T \gets CREATE-NEW-NODE()$
  \State $KEY(T) \gets x$
  \State $LEFT(T) \gets l$
  \State $RIGHT(T) \gets r$
  \State \Return $T$
\EndFunction
\end{algorithmic}

\subsection*{Definition of Splay heap in Haskell}

Since Splay tree is a special binary search tree, the definition of
them are same.

\lstset{language=Haskell}
\begin{lstlisting}
data STree a = E -- Empty
             | Node (STree a) a (STree a) -- left, element, right
               deriving (Eq, Show)
\end{lstlisting}

Translate the above algorithm into Haskell gets the following partition
program.

\begin{lstlisting}
partition :: (Ord a) => STree a -> a -> (STree a, STree a)
partition E _ = (E, E)
partition t@(Node l x r) y
    | x < y =
        case r of
          E -> (t, E)
          Node l' x' r' ->
              if x' < y then
                  let (small, big) = partition r' y in
                  (Node (Node l x l') x' small, big)
              else
                  let (small, big) = partition l' y in
                  (Node l x small, Node big x' r')
    | otherwise =
        case l of
          E -> (E, t)
          Node l' x' r' ->
              if y < x' then
                  let (small, big) = partition l' y in
                  (small, Node l' x' (Node r' x r))
              else
                  let (small, big) = partition r' y in
                  (Node l' x' small, Node big x r)
\end{lstlisting}

In a language which supports `pattern matching' such as Haskell,
Splay can be implemented in a very simple and straightforward style
by translating the figure \ref{fig:zig-zig}, \ref{fig:zig-zag} and
\ref{fig:zig} directly into patterns. Note that for each step,
there are two left-right symmetric cases.

\lstset{language=Haskell}
\begin{lstlisting}
-- splay by pattern matching
splay :: (Eq a) => STree a -> a ->STree a
-- zig-zig
splay t@(Node (Node (Node a x b) p c) g d) y =
    if x == y then Node a x (Node b p (Node c g d)) else t
splay t@(Node a g (Node b p (Node c x d))) y =
    if x == y then Node (Node (Node a g b) p c) x d else t
-- zig-zag
splay t@(Node (Node a p (Node b x c)) g d) y =
    if x == y then Node (Node a p b) x (Node c g d) else t
splay t@(Node a g (Node (Node b x c) p d)) y =
    if x == y then Node (Node a g b) x (Node c p d) else t
-- zig
splay t@(Node (Node a x b) p c) y = if x == y then Node a x (Node b p c) else t
splay t@(Node a p (Node b x c)) y = if x == y then Node (Node a p b) x c else t
-- otherwise
splay t _ = t
\end{lstlisting}

\subsection*{Definition of Splay heap in Scheme/Lisp}

Since Splay heap is essentially binary search tree. The definition
is same. In order to output the tree in in-order. we arrange it
as (left element right) format.

Some auxiliary functions to access left child, element, right
child and constructor are defined as same as in ref{skew-heap-def-lisp}.

The function which can partition the tree into 2 parts according to
a pivot value based on algorithm $PARTITION$ is defined as the following.
It's a bit complex, but not hard. We compared the pivot and the element
and traverse the tree based on binary search tree property. In case
there are two left (or right) children traversed, the tree is rotated
by splaying. It makes the tree more and more balanced.

\lstset{language=lisp}
\begin{lstlisting}
(define (partition t pivot)
  (if (null? t)
      (cons '() '())
      (let ((l (left t))
	    (x (elem t))
	    (r (right t)))
	(if (< x pivot)
	    (if (null? r)
		(cons t '())
		(let ((l1 (left r))
		      (x1 (elem r))
		      (r1 (right r)))
		  (if (< x1 pivot)
		      (let* ((p (partition r1 pivot))
			     (small (car p))
			     (big (cdr p)))
			(cons (make-tree (make-tree l x l1) x1 small) big))
		      (let* ((p (partition l1 pivot))
			     (small (car p))
			     (big (cdr p)))
			(cons (make-tree l x small) (make-tree big x1 r1))))))
	    (if (null? l)
		(cons '() t)
		(let ((l1 (left l))
		      (x1 (elem l))
		      (r1 (right l)))
		  (if (> x1 pivot)
		      (let* ((p (partition l1 pivot))
			     (small (car p))
			     (big (cdr p)))
			(cons small (make-tree l1 x1 (make-tree r1 x r))))
		      (let* ((p (partition r1 pivot))
			     (small (car p))
			     (big (cdr p)))
			(cons (make-tree l1 x1 small) (make-tree big x r))))))))))
\end{lstlisting}

Although there is no direct pattern matching language feature supporting in
Scheme/Lisp, it possible to provide a splay function by guard clause.
Compare to the Haskell one, it is a bit more complex.

\begin{lstlisting}
(define (splay l x r y)
  (cond ((eq? y (elem (left l))) ;; zig-zig
	 (make-tree (left (left l))
		    (elem (left l))
		    (make-tree (right (left l))
			       (elem l)
			       (make-tree (right l) x r))))
	((eq? y (elem (right r))) ;; zig-zig
	 (make-tree (make-tree (make-tree l x (left r))
			       (elem r)
			       (left (right r)))
		    (elem (right r))
		    (right (right r))))
	((eq? y (elem (right l))) ;; zig-zag
	 (make-tree (make-tree (left l) (elem l) (left (right l)))
		    (elem (right l))
		    (make-tree (right (right l)) x r)))
	((eq? y (elem (left r))) ;; zig-zag
	 (make-tree (make-tree l x (left (left r)))
		    (elem (left r))
		    (make-tree (right (left r)) (elem r) (right r))))
	((eq? y (elem l)) ;; zig
	 (make-tree (left l) (elem l) (make-tree (right l) x r)))
	((eq? y (elem r)) ;; zig
	 (make-tree (make-tree l x (left r)) (elem r) (right r)))
	(else (make-tree l x r))))
\end{lstlisting}

% ================================================================
%                 Basic heap operations
% ================================================================
\subsection{Basic heap operations}

There are two methods to implement basic heap operations for Splay
heap. One is by using $PARTITION$ algorithm we defined, the other
is to utilize a $SPLAY$ process, which can be realized in pattern matching
way in languages equipped with this feature.

\subsubsection{Insertion}

If using $PARTITION$ algorithm, once we want to insert a new $x$ into
a heap $T$, we can first partition it into two trees, $L$ and $R$. Where
L contains all nodes smaller than $x$, and $R$ contains all bigger ones.
We then construct a new node, with $x$ as the root and $L$, $R$ as children.

\begin{algorithmic}[1]
\Function{INSERT}{$T, x$}
  \State $(L, R) \gets PARTITION(T, x)$
  \State \Return $CREATE-NODE(L, x, R)$
\EndFunction
\end{algorithmic}

If there is a $SPLAY$ algorithm defined, the insert can be done in a
recursive way as the following.

\begin{algorithmic}[1]
\Function{INSERT'}{$T, x$}
  \If{$T = NIL$}
    \State \Return $CREATE-NODE(NIL, x, NIL)$
  \EndIf
  \If{$KEY(T) < x$}
    \State $LEFT(T) \gets INSERT'(LEFT(T), x)$
  \Else
    \State $RIGHT(T) \gets INSERT'(RIGHT(T), x)$
  \EndIf
  \State \Return $SPLAY(T, x)$
\EndFunction
\end{algorithmic}

\subsubsection*{Insertion in Haskell}

It's easy to translate the above algorithms into Haskell.

\lstset{language=Haskell}
\begin{lstlisting}
insert :: (Ord a) => STree a -> a -> STree a
insert t x = Node small x big where (small, big) = partition t x
\end{lstlisting}

And the pattern matching one is in recursive manner as below.

\begin{lstlisting}
insert' :: (Ord a) => STree a -> a -> STree a
insert' E y = Node E y E
insert' (Node l x r) y
    | x > y     = splay (Node (insert' l y) x r) y
    | otherwise = splay (Node l x (insert' r y)) y
\end{lstlisting}

\subsubsection*{Insertion in Scheme/Lisp}

By using partition method, when insert a new element to the Splay
heap, we use this element as the pivot to partition the tree.
After that we set this new element as the new root, the sub tree
contains all elements smaller than root as the left child, and
the others as the right child. Note that we intend not handle the
duplicated elements case, because it quite possible to contains
them in Splay heap.

\lstset{language=lisp}
\begin{lstlisting}
(define (insert t x)
  (let* ((p (partition t x))
	 (small (car p))
	 (big (cdr p)))
    (make-tree small x big)))
\end{lstlisting}

And if use splay function, the insert can be implemented straightforward
like binary search tree, except that we need apply splaying recursively
after that.

\begin{lstlisting}
(define (insert-splay t x)
  (cond ((null? t) (make-tree '() x '()))
	((> (elem t) x)
	 (splay (insert-splay (left t) x) (elem t) (right t) x))
	(else
	 (splay (left t) (elem t) (insert-splay (right t) x) x))))
\end{lstlisting}

\subsubsection{Verify how Splay improve the balance}
In order to show how Splaying improves the balance of binary search
tree, we first insert a list of ordered element to the tree and then
performs a large number of arbitrary node access.

A look-up algorithm is provided with Splaying operation inside.

\begin{algorithmic}[1]
\Function{LOOKUP}{$T, x$}
  \If{$KEY(T) = x$}
    \State \Return $T$
  \ElsIf{$KEY(T) > x$}
    \State $LEFT(T) \gets LOOKUP(LEFT(T), x)$
  \Else
    \State $RIGHT(T) \gets LOOKUP(RIGHT(T), x)$
  \EndIf
  \State \Return $SPLAY(T, x)$
\EndFunction
\end{algorithmic}

Translate this algorithm into Haskell yields the following program\footnote{$LOOKUP$ algorithm in other language is skipped.}.

\lstset{language=Haskell}
\begin{lstlisting}
lookup' :: (Ord a) => STree a -> a -> STree a
lookup' E _ = E
lookup' t@(Node l x r) y
    | x == y    = t
    | x > y     = splay (Node (lookup' l y) x r) y
    | otherwise = splay (Node l x (lookup' r y)) y
\end{lstlisting}

Next we can create a Splay heap by inserting sequence number from
1 to 10. After that, We random select from a number from this range,
and perform looking up 1000 times as below.

\begin{lstlisting}
testSplay = do
  xs <- sequence (replicate 1000 (randomRIO(1, 10)))
  putStrLn $ show (foldl lookup' t xs)
      where
        t = foldl insert' (E::STree Int) [1..10]
\end{lstlisting} %$

Run these test will make the tree quite balance. Below is an example
result. Figure \ref{fig:splay-result} shows the tree after splaying.

\begin{verbatim}
Node (Node (Node (Node E 1 E) 2 E) 3 E) 4 (Node (Node E 5 E) 6 (Node
(Node (Node E 7 E) 8 E) 9 (Node E 10 E)))
\end{verbatim}

\begin{figure}[htbp]
   \begin{center}
   	  \includegraphics[scale=0.5]{img/splay-tree.ps}
          \caption{Splaying helps improving the balance.} \label{fig:splay-result}
   \end{center}
\end{figure}


\subsubsection{Find minimum (top) and delete minimum (pop)}
Since Splay tree is just a special binary search tree, so the minimum
element is stored in the left most node. We need keep traversing
the left child.

\begin{algorithmic}[1]
\Function{FIND-MIN}{$T$}
  \If{$LEFT(T) = NIL$}
    \State \Return $KEY(T)$
  \Else
    \State \Return $FIND-MIN(LEFT(T))$
  \EndIf
\EndFunction
\end{algorithmic}

And for pop operation, the algorithm need keep traversing in left
and also remove the minimum element from the tree. In case there
are two left nodes traversed, a splaying operation should be performed.

\begin{algorithmic}[1]
\Function{DEL-MIN}{$T$}
  \If{$LEFT(T) = NIL$}
    \State \Return $RIGHT(T)$
  \ElsIf{$LEFT(LEFT(T)) = NIL$}
    \State $LEFT(T) \gets RIGHT(LEFT(T))$
    \State \Return $T$
  \Else
    \Comment{Splaying}
    \State $l \gets LEFT(T)$
    \State $r \gets CREATE-NEW-NODE()$
    \State $LEFT(r) \gets RIGHT(l)$
    \State $KEY(r) \gets KEY(T)$
    \State $RIGHT(r) \gets RIGHT(T)$
    \State $T' \gets CREATE-NEW-NODE()$
    \State $LEFT(T') \gets DEL-MIN(LEFT(l))$
    \State $KEY(T') \gets KEY(l)$
    \State $RIGHT(T') \gets r$
    \State \Return $T'$
  \EndIf
\EndFunction
\end{algorithmic}

Note that the find minimum and delete minimum algorithms both are
bound to $O(\lg N)$.

\subsubsection*{Find minimum (top) and delete minimum in Haskell}

When translate the above algorithms into Haskell, one option is to
use pattern matching.

\lstset{language=Haskell}
\begin{lstlisting}
findMin :: STree a -> a
findMin (Node E x _) = x
findMin (Node l x _) = findMin l

deleteMin :: STree a -> STree a
deleteMin (Node E x r) = r
deleteMin (Node (Node E x' r') x r) = Node r' x r
deleteMin (Node (Node l' x' r') x r) = Node (deleteMin l') x' (Node r' x r)
\end{lstlisting}

\subsubsection*{Find minimum (top) and delete minimum in Scheme/Lisp}

In Scheme/Lisp, finding minimum element means keep traversing to left;
while for max-heap, we need change the program to keep traversing to
right.

\lstset{language=lisp}
\begin{lstlisting}
(define (find-min t)
  (if (null? (left t))
      (elem t)
      (find-min (left t))))
\end{lstlisting}

And for delete minimum program, except for trivial case, splaying also
need be performed if we traverse in left twice.

\begin{lstlisting}
(define (delete-min t)
  (cond ((null? (left t)) (right t))
	((null? (left (left t))) (make-tree (right (left t)) (elem t) (right t)))
	(else (make-tree (delete-min (left (left t)))
			 (elem (left t))
			 (make-tree (right (left t)) (elem t) (right t)))))
\end{lstlisting}

\subsubsection{Merge}
Merge is another basic operation for heaps as it is widely used in Graph algorithms. By using $PARTITION$ algorithm, merge can be realized in $O(\lg N)$ time.

When merging two Splay trees, for non-trivial case, we can take the root element of the first tree as the new root element, then partition the second tree with the new root as the pivot value. After that we recursively merge
the children of the first tree with the partition result. This algorithm is shown as the following.

\begin{algorithmic}[1]
\Function{MERGE}{$T1, T2$}
  \If{$T1 = NIL$}
    \State \Return $T2$
  \Else
    \State $L \gets LEFT(T1)$
    \State $R \gets RIGHT(T1)$
    \State $k \gets KEY(T1)$
    \State $(L', R') \gets PARTITION(T2, k)$
    \State \Return $CREATE-NODE(MERGE(L, L'), k MERGE(R, R'))$
  \EndIf
\EndFunction
\end{algorithmic}

\subsubsection*{Merge two Splay heaps in Haskell}
In Haskell, we can handle the trivial and non-trivial case by pattern matching.

\lstset{language=Haskell}
\begin{lstlisting}
merge :: (Ord a) => STree a -> STree a -> STree a
merge E t = t
merge (Node l x r) t = Node (merge l l') x (merge r r')
    where (l', r') = partition t x
\end{lstlisting}

\subsubsection*{Merge two Splay heaps in Scheme/Lisp}

In Scheme/Lisp, we translate the above algorithm strictly as the following.

\lstset{language=lisp}
\begin{lstlisting}
(define (merge t1 t2)
  (if (null? t1)
      t2
      (let* ((p (partition t2 (elem t1)))
	     (small (car p))
	     (big (cdr p)))
	(make-tree (merge (left t1) small) (elem t1) (merge (right t1) big)))))
\end{lstlisting}

% ================================================================
%                 Heap sort
% ================================================================
\subsection{Heap sort}

Since the internal implementation of the Splay heap is completely
transparent to the heap interface, the heap sort algorithm can
be reused. It means that the heap sort algorithm is generic no
matter what the underground data structure is.

% ================================================================
%                 Short summary
% ================================================================
\section{Notes and short summary}

In this post, we reviewed the definition of binary heap, and adjust it
a bit so that as long as the heap property is maintained, all binary
representation of data structures can be used to implement binary heap.

This enable us not only limit to the popular implicit binary heap
by array, but also extend to explicit binary heaps including Leftist
heap, Skew heap and Splay heap. Note that, the implicit binary heap
by array is particularly convenient for imperative implementation
because it intense uses random index access which can be mapped to
a completely binary tree. It's hard to directly find functional
counterpart in this way.

However, by using explicit binary tree, functional implementation
can be easily achieved, most of them have $O(\lg N)$ worst case
performance, and some of them even reach $O(1)$ amortize time.
Okasaki in \cite{okasaki-book} shows good analysis of these data
structures.

In this post, Only pure functional realization for Leftist heap,
Skew heap, and Splay heap are explained, they are all possible to
be implemented in imperative way. I skipped them only for the
purpose of presenting comparable functional algorithm to the
implicit binary heap by array.

It's very natural to extend the concept from binary tree to
K-ary (K-way) tree, which leads to other useful heap concept such as
Binomial heaps, Fibonacci heaps and pairing heaps. I'll introduce
them in a separate post later.

% ================================================================
%                 Appendix
% ================================================================
\section{Appendix} \label{appendix}
%\appendix
All programs provided along with this article are free for
downloading.

\subsection{Prerequisite software}
GNU Make is used for easy build some of the program. For C++ and ANSI C programs,
GNU GCC and G++ 3.4.4 are used.
For Haskell programs GHC 6.10.4 is used
for building. For Python programs, Python 2.5 is used for testing, for
Scheme/Lisp program, MIT Scheme 14.9 is used.

all source files are put in one folder. Invoke 'make' or 'make all'
will build C++ Program.

There is no separate Haskell main program module, however, it is possible to run the program in GHCi.

\begin{itemize}
\item bheap.hpp. This is the C++ source file contains binary heap defintion and functions, There are two types of approach, one is the ``reference + size'' way, the other is ``range'' representation.

\item test.cpp. This is the main C++ program to test bheap.hpp module.

\item bheap.py. This is the Python source file for binary heap implementation. It's a self-contained program with test cases embedded. run it directly will performs all test cases. It is also possible to import it as a module.

\item LeftistHeap.hs. This is the Haskell program for Leftist Heap with some simple test cases as well. It can be loaded to GHCi directly.

\item SkewHeap.hs. This is the Haskell program for Skew heap defintion. Some very simple test cases are provided.

\item SplayHeap.hs. This is the Haskell proram for Splay heap definition.

\item lefitst.scm. This is the Scheme/Lisp program for Leftist heap.

\item skewheap.scm. This is the Scheme/Lisp program for Skew heap. Same as leftist.scm, some gerneric functions are reused.

\item genheap.scm. This is the Scheme/Lisp general heap function utilities which are all same for varies of heaps. It can be overwritten afterwards.

\item splayheap.scm. This is the Scheme/Lisp program for Splay heap definition.
\end{itemize}

download position: http://sites.google.com/site/algoxy/btree/bheap.zip

\begin{thebibliography}{99}

\bibitem{CLRS}
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein. ``Introduction to Algorithms, Second Edition''. The MIT Press, 2001. ISBN: 0262032937.

\bibitem{wiki-heap}
Heap (data structure), Wikipedia. http://en.wikipedia.org/wiki/Heap\_(data\_structure)

\bibitem{wiki-heapsort}
Heapsort, Wikipedia. http://en.wikipedia.org/wiki/Heapsort

\bibitem{okasaki-book}
Chris Okasaki. ``Purely Functional Data Structures''. Cambridge university press, (July 1, 1999), ISBN-13: 978-0521663502

\bibitem{rosetta-heapsort}
Sorting algorithms/Heapsort. Rosetta Code. http://rosettacode.org/wiki/Sorting\_algorithms/Heapsort

\bibitem{wiki-leftist-tree}
Leftist Tree, Wikipedia. http://en.wikipedia.org/wiki/Leftist\_tree

\bibitem{brono-book}
Bruno R. Preiss. Data Structures and Algorithms with Object-Oriented Design Patterns in Java. http://www.brpreiss.com/books/opus5/index.html

\bibitem{TAOCP}
Donald E. Knuth. ``The Art of Computer Programming. Volume 3: Sorting and Searching.''. Addison-Wesley Professional;
2nd Edition (October 15, 1998). ISBN-13: 978-0201485417. Section 5.2.3 and 6.2.3

\bibitem{wiki-skew-heap}
Skew heap, Wikipedia. http://en.wikipedia.org/wiki/Skew\_heap

\bibitem{self-adjusting-heaps}
Sleator, Daniel Dominic; Jarjan, Robert Endre. ``Self-adjusting heaps'' SIAM Journal on Computing 15(1):52-69. doi:10.1137/0215004 ISSN 00975397 (1986)

\bibitem{wiki-splay-tree}
Splay tree, Wikipedia. http://en.wikipedia.org/wiki/Splay\_tree

\bibitem{self-adjusting-trees}
Sleator, Daniel D.; Tarjan, Robert E. (1985), ``Self-Adjusting Binary Search Trees'', Journal of the ACM 32(3):652 - 686, doi: 10.1145/3828.3835

\bibitem{NIST}
NIST, ``binary heap''. http://xw2k.nist.gov/dads//HTML/binaryheap.html

\end{thebibliography}

\ifx\wholebook\relax \else
\end{document}
\fi
